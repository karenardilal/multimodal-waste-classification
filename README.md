# multimodal-waste-classification
ENEN 645 Assignment 2 Repo: Garbage classification combining image features with filename text information, trained and evaluated on TALC.

This project implements a multimodal deep learning pipeline for 4-class waste classification (Black, Blue, Green, TTR). The data is transformed using several methods of data augmentation to improve generalization, including geometric transformations, color perturbations, and Random Erasing to simulate occlusion. The architecture follows a two-branch multimodal design: a pretrained ResNet50 image backbone and a TF-IDF text branch derived from filenames, as we proposed in Assignment 1. The text branch is not used directly (raw); instead, TF-IDF vectors are passed through a compact Multi-Layer Perceptron (MLP), which projects the high-dimensional sparse TF-IDF representation into a learned dense embedding space. This MLP consists of a linear transformation, ReLU nonlinearity, and dropout regularization, allowing the model to learn nonlinear interactions between text tokens while reducing dimensionality and overfitting. The resulting text embedding is then concatenated with the 2048-dimensional image features extracted from ResNet50, enabling feature-level fusion prior to final classification. To prevent data leakage, the TF-IDF vectorizer is fit on the training set only and validation/test splits remain untouched throughout preprocessing.

The training loop incorporates a two-stage transfer learning strategy: an initial warm-up phase where the ResNet backbone is frozen and only the text MLP and classifier are trained, followed by selective fine-tuning of higher ResNet layers (layer3 and layer4, because we want them to capture more details from phone pictures vs ImageNet web-based pictures) using discriminative learning rates (carefully to avoid catastrophic forgetting). This stabilizes convergence, protects pretrained visual representations from early corruption, and allows gradual adaptation to the domain-specific waste dataset. Class imbalance is handled explicitly using a "Weighted Random Sampler" applied only to the training loader, while evaluation emphasizes balanced accuracy and macro F1 to ensure equitable performance across minority classes. Additional robustness measures include AdamW for decoupled weight decay, early stopping and ReduceLROnPlateau scheduling based on validation balanced accuracy, clean checkpointing of the best model, controlled GPU execution settings for cluster stability, and generation of a confusion matrix and misclassification grid for error analysis.

The final model achieves strong and well-balanced test performance, obtaining approximately 85.96% accuracy, 85.33% balanced accuracy, 85.56% macro F1. Performance is particularly strong for the Green class, while residual errors are primarily concentrated between visually similar categories, notably Black and Blue, suggesting that inter-class visual similarity remains the dominant source of misclassification (this can be observed in the output figures).
